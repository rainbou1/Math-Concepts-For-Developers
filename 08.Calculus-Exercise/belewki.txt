Дефиниция на поле:
https://en.wikipedia.org/wiki/Field_(mathematics)
Зад поле стои нещо с определени характеристики

Classic definition
Formally, a field is a set F together with two binary operations on F called addition and multiplication.[1] A binary operation on F is a mapping F × F → F, that is, a correspondence that associates with each ordered pair of elements of F a uniquely determined element of F.[2][3] The result of the addition of a and b is called the sum of a and b, and is denoted a + b. Similarly, the result of the multiplication of a and b is called the product of a and b, and is denoted ab or a ⋅ b. These operations are required to satisfy the following properties, referred to as field axioms (in these axioms, a, b, and c are arbitrary elements of the field F):

Associativity of addition and multiplication: a + (b + c) = (a + b) + c, and a ⋅ (b ⋅ c) = (a ⋅ b) ⋅ c.
Commutativity of addition and multiplication: a + b = b + a, and a ⋅ b = b ⋅ a.
Additive and multiplicative identity: there exist two distinct elements 0 and 1 in F such that a + 0 = a and a ⋅ 1 = a.
Additive inverses: for every a in F, there exists an element in F, denoted −a, called the additive inverse of a, such that a + (−a) = 0.
Multiplicative inverses: for every a ≠ 0 in F, there exists an element in F, denoted by a−1 or 1/a, called the multiplicative inverse of a, such that a ⋅ a−1 = 1.
Distributivity of multiplication over addition: a ⋅ (b + c) = (a ⋅ b) + (a ⋅ c).
An equivalent, and more succinct, definition is: a field has two commutative operations, called addition and multiplication; it is a group under addition with 0 as the additive identity; the nonzero elements are a group under multiplication with 1 as the multiplicative identity; and multiplication distributes over addition.

Even more succinctly: a field is a commutative ring where 0 ≠ 1 and all nonzero elements are invertible under multiplication.

Distributivity - умножението може да бъде разпределено спрямо събиране

Някаква колекция от елементи, ако може да отговаря на всички тези свойства, можем да наречем поле.
Защо е важно - заради принципа на абстракция
Каква абстракция ни е полезна - полетата са много полезни заради полетата от числа - тъй като това е минималното определение, което върши работа за всичките операции - събиране и умножение и обратните операции...и свойството дистрибутивност на умножението. 
Което означава, че всяко нещо, което научим за числата, можем да прилагаме и в други ситуации.

Целите числа не са поле.

Колкото повече ограничение даваме за нещо, толкова повече знаем за него.

Равнина е двумерна линейна структура.
Какво означава нещо да е линейно - за него означава да са изпълени принципите, че: 
f(a) + f(b) = f(a + b) за всяко a, b - принципа на суперпозицията
lambda * f(a) = f(lambda * a)

където a, b са елементи от множество, lambda e скалар
Това нещо в две измерения представлява права линия през точката 0.
За линейната алгебра това е афинна функция.

3x1 + 2
3x1 + 7x2 + 2
4x1 - 5x2 + 0.5x3 + 3

Всичките функции са линейни спрямо всяка една входна променливи.
Всичките тези структури се наричат равнини.
В едномерно пространство - едномерна равнина - права линия
Двумерна равнина - равнина.
За повече измерения изпозваме термина хиперравнина. - 326-мерна хиперравнина.
Математическата дефиниция е линейна комбинация от базисните вектори

https://paperswithcode.com/
Какво е логаритмична ос? От ChatGPT:
Логаритмична ос на графика е ос, която е скалата, основата на която е логаритъмна функция. Обикновено се използва логаритъм с основа 10 (логаритъм от 10), наричан десетичен логаритъм, или с основа е (натурален логаритъм).



От задачите:
=============================
Производна на функция в точка е свързана с допирателната на функцията в тази точка.
Но не би трябвало да е допирателната.
Производната на функция в една точка е число.
Производната на функция изобщо е друга функция.

Същото важи и за неопределен интеграл - който е функция евентуално на същата променлива.
Имаме и определен интеграл който е константа спрямо променливата която му подаваме.

Тоест в една точка е число, като го направим безброй много пъти получаваме функция.

Можем да сметнем производната във всяка една точка.

Неперово число е "е"

Производни на степенни функции

Problem 4:
Производните се смятат много трудно числено. Защото производна означава малка разлика в изхода разделена на крайна разлика на входа. Обаче заради крайния брой битове, в който събираме числата, имаме ограничение. Например 10 на -30-та ще бъде запазена като 0.
Правило на трапеците
За проверка на резултати можем да ползваме WolframAlpha
Смяна на променливи.

Problem 7:
Тъй като имаме F = ma = от ускорение с интегриране получаваме скорост, след това с интегриране позиция. Така за всяка частица с известна маса може да определим къде ще се намира в даден момент.
Ужасно много неща се моделират с Particle Systems.

Диференциално уравнение - уравнения, в които участват производни на функцията, която търсим.
Ако търсим производна на функция, значи търсим скоростта и.
Например търсим коя е тая функция, която скорост на нарастване e df/dx = 5x^2.
Тоест знаем скоростта на нещо и питаме какво е то. Това има милиарди приложения.
Най-популярното диференциално е wave equation.
https://en.wikipedia.org/wiki/Wave_equation

 Описанието на вълните представляват диференциални уравнения.


Приложения на мат. анализ:
1.Граници 
Едно от приложенията на мат. анализ е да можем да кажем колко бързо расте една функция.
При компютрите се интересуваме колко бързо работи една функция. Под функция се разбира парче код.
Интересно е защото искаме да разберем дали кода въобще ще завърши. Това е halting problem. 
След това е интересно за колко време ще завърши.
Интересуваме се от поведението на функцията във времето...Интересуваме се с колко по-бавна става функцията с времето (ето производна)
n - големина на вход на функцията, големината на задачата.
питаме какво става, когато n клони към безкрайност. Тоест по какъв начин ще расте времето за изпълнение на нашта функция, когато n клони към безкрайност.
Появяват се класове сложности. - complexity class. С O (big notation) означаваме клас сложност.
https://www.google.bg/search?sca_esv=c45adb40d50f9228&sca_upv=1&q=big+o+notation&uds=ADvngMgNG4qWEcyOv6mZ7d9R1NXiGjcjcZnsLF5DUWZpMRQx7C9a6K8lPKRWEgbGCYaAELY3yeLBTGmEyF2ACtuCLa81R3RakCue2MxD7NXOtrQrYQRiq5-lnD_H1e3N9XVdeenVPA1t5PsuzXQvaN4EN59VPPT8CsgCLzP9r7maMldn_rRMyTRmsuri3J_J4eHsV9l_SgY9NoalwXeqYcDEEKLVlnZ695DQBhtdUZWtXEhL2rAXs_PE0H7C8SsuhBzSaXagkDHcHsVnXj4KoCoMk08gy3S58cgbuak-J3L78cc83lFYVTiuJAgs7rNGIsmwXRuppmY5hBe8256ooBPIlrTshQ95yw&udm=2&prmd=ivnbz&sa=X&ved=2ahUKEwii0vjfoIWGAxUOXvEDHS9GD78QtKgLegQIChAB&biw=1488&bih=742&dpr=1.25#vhid=lk1jUZ97bTgGXM&vssid=mosaic

Класовете са константа, логаритъм от n и т.н.
O(logN) алгоритъм - например в търсене в сортиран списък
O(n^2) - за всяко n имаме n на брой итерации. Или 2 вложени цикъла.
Умножението на матрици е O(n^3)
O(1) - Сложността и времето на изпълнение не зависи от размера на входните данни. - например търсене в хештаблица (dictionary в Python)

Това е приложение на математическия анализ

Измерение е нещо, което измерваме.

Тестване. Когато дебъгваме имаме да тестваме много на брой функции.
Например когато имаме някаква метрика на модела, като MSE. 
Mean Squared Error (MSE) - представлява функция, в която участват предсказаните резултати и реалните резултати.
Но предсказаните резултатите зависят от нещо...от данни (вектор) и много настройки на модела.
Така много неща зависят от много други неща и т.н.
https://www.google.bg/search?q=neural+network&sca_esv=e11fd8c8d000e92d&sca_upv=1&udm=2&biw=1488&bih=742&ei=sGo_Zp_jDOiL7NYPlImTwAY&ved=0ahUKEwif-7zN0oWGAxXoBdsEHZTEBGgQ4dUDCBA&uact=5&oq=neural+network&gs_lp=Egxnd3Mtd2l6LXNlcnAiDm5ldXJhbCBuZXR3b3JrMgUQABiABDIEEAAYHjIEEAAYHjIEEAAYHjIEEAAYHjIEEAAYHjIEEAAYHjIEEAAYHjIEEAAYHjIEEAAYHkiLDVCUC1iUC3ACeACQAQCYAWGgAWGqAQExuAEDyAEA-AEBmAIDoAJxwgIHEAAYgAQYE5gDAIgGAZIHAzIuMaAH6QQ&sclient=gws-wiz-serp#vhid=SEEMeUsxiBOGGM&vssid=mosaic

Например при невронни мрежи.
Резултатът накрая зависи от предния, който зависи от предния. ...който зависи от предния, който зависи от входа...
На изхода имате функция от функция от функция...на входа.
Това прави дебъгването и разбирането много сложно.
Трябва да внимаваме много кое от какво зависи.
Зависимостите тук са показани като черти. В Tensorboard като стрелки.

Решаване на задачите:
====================================
Problem 1:
numpy.vectorize - The numpy.vectorize function is a tool in NumPy that allows you to "vectorize" a Python function, meaning it can operate element-wise on arrays rather than just scalar inputs.

Производната представлява някаква граница, при която имаме графика, начална точка, крайна точка и права която ги свързва.
Тази права линия е в общия линии секуща.
Когато крайната точка мърдаме до началната, в някакъв момент разликата ще стане толкова малка, че тези 2 точки са на практика една върху друга и това е допирателната права в тази точка (граничен случай при безкрайно малки разлики между A и B).

граница 0,(9) = 1 (0, 9 в период)

Problem 2:
numpy записва числата в ограничен брой битове. Затова в по-голям диапазон се получи проблем.

Problem 3:
Колко е производната на 2^x? Производна на a^x = a^x * lna.
Формулата на Ойлер разказва за връзката между експоненциалнен растеж и периодични функции.
Разглеждаме втора производна на функция да е = минус функцията. Връзката между sin, cos и е, може да се разгледа в Problem 5. 

От Дискорд:
"Производните, the Derivatives, получават по-голямо внимание в Machine Learning. Диференцирането (намиране производната на функция) означава разлика. Разлика означава малка грешка, а малка грешка може да ни каже „сбъркал съм с малко“.
Как мога да направя това, което съм в момента, малко по-добро? Ако направя стъпка в посока обратна на моята най-голяма грешка, би трябвало да направя стъпка в посока най-правилното решение, което виждам в момента.
Така работи Machine Learning, локална оптимизация. Ако се намирам някъде в една loss function, в ML algorithm… мога да разбера коя е най-голямата грешка, която мога да направя – къде производната е най-голяма, накъде сочи градиента – и да направя точно обратното нещо, за една стъпка. Ако направя същия алгоритъм много на брой пъти, получавам Gradient Descent. И в някакъв момент, когато разбера, че съм в минимум – няма място за още по-малка грешка – значи съм оптимизирал моя ML algorithm. Това е идеята зад Gradient Descent." ЙД

Problem 4:
https://www.mathwords.com/t/trapezoid_rule.htm
Когато променяме координатите, променяме абсолютно всичко
Ред на тейлър - радиус на сходимост